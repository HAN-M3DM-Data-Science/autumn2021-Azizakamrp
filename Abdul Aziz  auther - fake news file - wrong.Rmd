---
title: "Assigment - Naive Bayes DIY"
author:
  - Abdul Aziz - Author
  - Ayoub Rabii - Reviewer

date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   html_notebook:
    toc: true
    toc_depth: 2
---

```{r}
library(tidyverse)
library(tm)
library(caret)
library(wordcloud)
library(e1071)
```

## Business Understanding
To find articles online is easy, however finding a credible one can be a bit harder. The following model can pinpoint words that usually show in fake texts , this will help check other fake news articles

## Data Understanding
For this data to be understood, we upload the articles.

```{r}
url <- "https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/assignments/master/datasets/NB-fakenews.csv"
rawDF <- read_csv(url)
```
```{r}
head(rawDF)
```
The label column has to be categorized so that all the fake articles can be recognized.
```{r}
rawDF$text <- rawDF$label %>% factor %>% relevel("1")
class(rawDF$label)
```

## Data Preparation
Curpus function will be used to to make the process easier.
```{r}
rawCorpus <- Corpus(VectorSource(rawDF$label))
inspect(rawCorpus[1])
```

In this step, we will ignore uppercase letters and numbers.

```{r}
cleanCorpus <- rawCorpus %>% tm_map(tolower) %>% tm_map(removeNumbers)
cleanCorpus <- cleanCorpus %>% tm_map(removeWords, stopwords()) %>% tm_map(removePunctuation)
cleanCorpus <- cleanCorpus %>% tm_map(stripWhitespace)
tibble(Raw = rawCorpus$content[1], Clean = cleanCorpus$content[1])
```
making a table will help us know if certain words were repeated more than others.

```{r}
cleanDTM <- cleanCorpus %>% DocumentTermMatrix
inspect(cleanDTM)
```
The data is going to be slit into 75% training and 25% testing
```{r}
set.seed(1234)
trainIndex <- createDataPartition(rawDF$label, p = .90, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)
```

In this step we create datasets
```{r}
trainDF <- rawDF[trainIndex, ]
testDF <- rawDF[-trainIndex, ]
```
```{r}
trainCorpus <- cleanCorpus[trainIndex]
testCorpus <- cleanCorpus[-trainIndex]

trainDTM <- cleanDTM[trainIndex, ]
testDTM <- cleanDTM[-trainIndex, ]
```
for a word to be recognized as a very repeated words, it has to show up at least 1000 times.

```{r}
freqWords <- trainDTM %>% findFreqTerms(100)
trainDTM <-  DocumentTermMatrix(trainCorpus, list(dictionary = freqWords))
testDTM <-  DocumentTermMatrix(testCorpus, list(dictionary = freqWords))
```
Words will always be classified as long as they are available.

```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0) %>% factor(levels = c(0,1), labels = c("No", "Yes"))
}

nColsDTM <- dim(trainDTM)[2]
trainDTM <- apply(trainDTM, MARGIN = 2, convert_counts)
testDTM <- apply(testDTM, MARGIN = 2, convert_counts)

head(trainDTM[,1:10])
```
## Modeling
We are set to create the model.
```{r}
nbayesModel <-  naiveBayes(trainDTM, trainDF$label, laplace = 1)
```
```{r}
predVec <- predict(nbayesModel, testDTM)
confusionMatrix(predVec, testDF$label, positive = "1", dnn = c("Prediction", "True"))
```


## Evaluation and Deployment
Since the accuracy is not high, this model isnt the best fit for this task.